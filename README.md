
# Voice Search Enhancement Using LLMs

This repository, `voice-search-enhancement-using-llms`, aims to leverage new Large Language Models (LLMs) and concepts to enhance voice search, especially on e-commerce platforms. The primary focus is on the ability to extract relevant results based on long, conversational audio queries.

## Table of Contents
- [Context](#context)
- [Data Collection and Pre-processing](#data-collection-and-pre-processing)
- [Backend Script](#backend-script)
  - [Taking the User's Audio Input](#taking-the-users-audio-input)
  - [Converting the Transcription into Crisp Queries](#converting-the-transcription-into-crisp-queries)
  - [Getting and Cleaning the Outputs](#getting-and-cleaning-the-outputs)
  - [High-level Workflow](#high-level-workflow)
- [App Deployment](#app-deployment)
- [Usage](#usage)
- [Future Scopes](#future-scopes)

## Context
Search has been, from the very advent of internet marketplaces, especially in the e-commerce sector, a topic of great research and development, showcasing consistent scopes of improvement as technologies continue to rise. However, as the internet at this point in time has come to spread to the remotest of places, the diversity of the audience on these platforms has significantly increased as well. With a major portion of this audience being taken up by a group of nascent users, it has become essential to introduce more flexibility to the existing search systems to make product browsing easy and convenient. The recent rise in the NLP (Natural Language Processing) abilities of LLMs (Large Language Models), particularly in terms of conversational and multimodal capabilities, shows great promise to introduce such flexibility to the users in the form of voice-based conversational search query refinement. This project aims at achieving the same via a simple Streamlit application. 

## Data Collection and Pre-processing
Before beginning to build the project, the first and foremost action item was to find an appropriate e-commerce dataset with a diverse product catalogue to test the ability of the model to precisely fetch relevant results. Upon much research, I found the Flipkart (a prominent e-commerce platform in India) product catalogue on Data World's website (https://data.world/promptcloud/product-details-on-flipkart-com). The dataset was in a CSV format, and had about 20K data points (products) with 15 columns (attributes) consisting of multiple information on each product such as its purchase URL, category tree, price, etc.  

Once the dataset was finalized, it was now time to clean it for further processing. By leveraging simple data cleaning techniques, I was able to create a cleaner version of the CSV. 
Next, since I was trying to leverage an index-based search instead of a simple CSV search, it was time to convert the data into a search index, by embedding the data associated with each product in the dataset. For this purpose, I decided to go ahead with the FAISS (Facebook AI Similarity Search) library, since the size of the data was small and creating an entire vector database (using libraries such as Pinecone), would have been meaningless. For creating the index, I first converted my CSV into a JSON file for the ease of conversion. Next, for each item (product) in the newly-created JSON file, I kept only the relevant information (name, description, URL, etc.) and encoded the rest with the help of the 'all-MiniLM-L6-v2' model offered by the Sentence Transformers library in Python, and created embeddings for each product. 

Once the embeddings were ready, I fed them to a flat FAISS index and saved the index file locally. The rest of the functions, particularly product search, were carried out on this index file.
For your reference, these data pre-processing and conversion scripts can be found in the Misc folder of the project directory.  

## Backend Script

### Taking the User's Audio Input
For this project, the spinal element was to be able to take the user's input in an audio format and transcribe it properly before searching for pivotal elements. For this, I leveraged the following Python libraries: torch, torchaudio, pyaudio, and wave for taking and managing audio input, and of course, transformers for converting the audio into a text transcription. The audio libraries enabled me to record the user's audio input with the host's audio input devices, automatically stop the recording after detecting a continuous silence for a buffer period (set to 4 seconds), save the audio file (as a temp file) in the local directory till it was transcribed, and finally delete it once the transcription was done. 
The audio file was transcribed to text with the help of HuggingFace and OpenAI's Whisper ('whisper-small') model leveraged using the transformers library in Python. The model conveniently converted the audio content into text, which was pushed further into the pipeline. 
These actions were taken care of within the speech_to_text.py script, and the model was initiated in the app.py script.

### Converting the Transcription into Crisp Queries
Once the audio transcription was done, it was time to convert it to crisp and concise queries that FAISS's search functionality could understand and interpret. For the same, I leveraged Meta's Llama3-70b-8192 model on Groq API by using prompt engineering techniques for the system prompt (QUARRY PROMPT) and passing the transcription as the user prompt to the LLM instance. The detailed system prompt ensured the extraction of key elements from the user's audio transcription and its conversion to a proper format for FAISS. The temperature and the top-p values of the LLM instance were set to low values to ensure less entropy in the generated query. 

### Getting and Cleaning the Outputs
Once the crisp and concise query for the product catalogue index (locally saved index file) was ready, the query was passed to the index search functionality of the FAISS library, which then searched the index and returned top-k (k=5) relevant and matching results, in a JSON format. 
This output JSON format, however, would be difficult for the user to read directly and grasp important information. For this, I once again leveraged another LLM instance with the same properties as the first one, and cleaned the generated JSON into a clean, readable format. The system prompt for this was the RE-SEARCHER PROMPT. 

The steps covered within the previous two sections were collectively carried out with the collaborative functioning of the prompts.py, get_search_dict.py, and query_processing.py scripts. 

### High-level Workflow
![Workflow Image](./Model_Workflow.png)

The high-level workflow is illustrated above and can be summarized as follows:
1. **User Input**: The user's voice input is taken and checked for silence.
2. **Transcription**: If silence is detected for more than 4 seconds, the input is sent to OpenAI's Whisper model for speech-to-text conversion.
3. **Query Conversion**: The transcribed text is processed by the QUARRY PROMPT to convert it into a concise search query.
4. **FAISS Search**: The concise query is used to search the FAISS index.
5. **Result Cleaning**: The search results are cleaned and prettified using the RE-SEARCHER PROMPT.
6. **User Output**: The cleaned results are presented to the user.

## App Deployment
With the completion of the backend pipeline, I moved to deploy the script with a user-friendly and minimal interface using the Streamlit library in Python. Adding some simple visual elements, and arranging the product information in a readable way, I was able to make a simple UI for the model and deploy it with the help of Streamlit. For the styles, I created a styles.css file, and finally brought it all together in the app.py file to run and test.

## Usage
If you wish to see the app in action, you can run it locally using the following commands:

### For Windows
```bash
streamlit run app.py
```

### For MacOS/Linux
```bash
streamlit run app.py
```

Make sure to update the keys in the .env files with the real keys, depending on whether you're using OpenAI or Groq.

## Future Scopes
The current project just covers a particular use case of enhancing voice-based search queries, which is translating conversational audio queries (in English). Future implementations could include the functionality of multilingual inputs and the ability to translate them to appropriate search queries, being able to converse back in audio format, etc. As LLM techniques continue to improve, we can expect search technologies to experience an upgrade as well.
